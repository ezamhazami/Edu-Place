{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34b633ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU name: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x = torch.rand(10000, 10000, device=device)  # allocated on GPU\n",
    "print(x.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8449de37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.3.23)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain) (0.3.54)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain) (0.3.32)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain) (2.11.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Collecting langchain-core<1.0.0,>=0.3.51 (from langchain)\n",
      "  Downloading langchain_core-0.3.74-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community)\n",
      "  Downloading aiohttp-3.12.15-cp313-cp313-win_amd64.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community) (9.1.2)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy>=2.1.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community) (2.2.4)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.9-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading frozenlist-1.7.0-cp313-cp313-win_amd64.whl.metadata (19 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading multidict-6.6.4-cp313-cp313-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading propcache-0.3.2-cp313-cp313-win_amd64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading yarl-1.20.1-cp313-cp313-win_amd64.whl.metadata (76 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting langsmith>=0.1.125 (from langchain-community)\n",
      "  Downloading langsmith-0.4.16-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (4.13.2)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (24.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith>=0.1.125->langchain-community) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Using cached mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n",
      "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.5/2.5 MB 17.3 MB/s eta 0:00:00\n",
      "Downloading langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/1.0 MB 25.4 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.12.15-cp313-cp313-win_amd64.whl (449 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
      "Downloading langchain_core-0.3.74-py3-none-any.whl (443 kB)\n",
      "Downloading langchain_text_splitters-0.3.9-py3-none-any.whl (33 kB)\n",
      "Downloading langsmith-0.4.16-py3-none-any.whl (375 kB)\n",
      "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.7.0-cp313-cp313-win_amd64.whl (43 kB)\n",
      "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading multidict-6.6.4-cp313-cp313-win_amd64.whl (45 kB)\n",
      "Downloading propcache-0.3.2-cp313-cp313-win_amd64.whl (40 kB)\n",
      "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading yarl-1.20.1-cp313-cp313-win_amd64.whl (86 kB)\n",
      "Using cached mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Installing collected packages: python-dotenv, propcache, mypy-extensions, multidict, marshmallow, httpx-sse, frozenlist, attrs, aiohappyeyeballs, yarl, typing-inspect, aiosignal, pydantic-settings, langsmith, dataclasses-json, aiohttp, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.3.32\n",
      "    Uninstalling langsmith-0.3.32:\n",
      "      Successfully uninstalled langsmith-0.3.32\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.54\n",
      "    Uninstalling langchain-core-0.3.54:\n",
      "      Successfully uninstalled langchain-core-0.3.54\n",
      "  Attempting uninstall: langchain-text-splitters\n",
      "    Found existing installation: langchain-text-splitters 0.3.8\n",
      "    Uninstalling langchain-text-splitters-0.3.8:\n",
      "      Successfully uninstalled langchain-text-splitters-0.3.8\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.3.23\n",
      "    Uninstalling langchain-0.3.23:\n",
      "      Successfully uninstalled langchain-0.3.23\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 attrs-25.3.0 dataclasses-json-0.6.7 frozenlist-1.7.0 httpx-sse-0.4.1 langchain-0.3.27 langchain-community-0.3.27 langchain-core-0.3.74 langchain-text-splitters-0.3.9 langsmith-0.4.16 marshmallow-3.26.1 multidict-6.6.4 mypy-extensions-1.1.0 propcache-0.3.2 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0 yarl-1.20.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install langchain langchain-community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a04a56e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.3.16.tar.gz (50.7 MB)\n",
      "     ---------------------------------------- 0.0/50.7 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 3.4/50.7 MB 20.2 MB/s eta 0:00:03\n",
      "     ------ --------------------------------- 8.7/50.7 MB 22.1 MB/s eta 0:00:02\n",
      "     ---------- ---------------------------- 13.4/50.7 MB 22.4 MB/s eta 0:00:02\n",
      "     --------------- ----------------------- 20.2/50.7 MB 25.9 MB/s eta 0:00:02\n",
      "     --------------------- ----------------- 28.0/50.7 MB 27.7 MB/s eta 0:00:01\n",
      "     ------------------------- ------------- 32.8/50.7 MB 27.3 MB/s eta 0:00:01\n",
      "     --------------------------- ----------- 35.4/50.7 MB 25.2 MB/s eta 0:00:01\n",
      "     ----------------------------- --------- 38.0/50.7 MB 23.9 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 40.9/50.7 MB 22.5 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 42.7/50.7 MB 20.9 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 44.3/50.7 MB 19.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 46.1/50.7 MB 18.7 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 48.2/50.7 MB 18.0 MB/s eta 0:00:01\n",
      "     --------------------------------------  50.6/50.7 MB 17.6 MB/s eta 0:00:01\n",
      "     --------------------------------------- 50.7/50.7 MB 17.2 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from llama-cpp-python) (4.13.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from llama-cpp-python) (2.2.4)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from llama-cpp-python) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): still running...\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.16-cp313-cp313-win_amd64.whl size=6726344 sha256=b44595e80a5101131c4db137888649293a16e8283ffd423e5ebd56fe8bde7ad7\n",
      "  Stored in directory: c:\\users\\asus\\appdata\\local\\pip\\cache\\wheels\\af\\3a\\b6\\445d9f4ccadd3ed923d55af8f055f2ccd217c66f09c834f0d8\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: diskcache, llama-cpp-python\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.16\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bdeb074",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 195 tensors from C:\\Users\\ASUS\\Downloads\\Phi-3-mini-4k-instruct-fp16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
      "llama_model_loader: - kv   1:                               general.name str              = Phi3\n",
      "llama_model_loader: - kv   2:                        phi3.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                      phi3.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   4:                   phi3.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv   5:                           phi3.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  phi3.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:               phi3.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   8:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv   9:                  phi3.rope.dimension_count u32              = 96\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 32000\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  130 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = F16\n",
      "print_info: file size   = 7.12 GiB (16.00 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control-looking token:  32007 '<|end|>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: control-looking token:  32000 '<|endoftext|>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "load: printing all EOG tokens:\n",
      "load:   - 32000 ('<|endoftext|>')\n",
      "load:   - 32007 ('<|end|>')\n",
      "load: special tokens cache size = 67\n",
      "load: token to piece cache size = 0.1690 MB\n",
      "print_info: arch             = phi3\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 4096\n",
      "print_info: n_embd           = 3072\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 32\n",
      "print_info: n_rot            = 96\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 96\n",
      "print_info: n_embd_head_v    = 96\n",
      "print_info: n_gqa            = 1\n",
      "print_info: n_embd_k_gqa     = 3072\n",
      "print_info: n_embd_v_gqa     = 3072\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 8192\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 4096\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 3B\n",
      "print_info: model params     = 3.82 B\n",
      "print_info: general.name     = Phi3\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32064\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 32000 '<|endoftext|>'\n",
      "print_info: EOT token        = 32007 '<|end|>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: PAD token        = 32000 '<|endoftext|>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 32000 '<|endoftext|>'\n",
      "print_info: EOG token        = 32007 '<|end|>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (f16) (and 194 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  7288.51 MiB\n",
      "....................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 2048\n",
      "llama_context: n_ctx_per_seq = 2048\n",
      "llama_context: n_batch       = 64\n",
      "llama_context: n_ubatch      = 8\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 2048 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   768.00 MiB\n",
      "llama_kv_cache_unified: size =  768.00 MiB (  2048 cells,  32 layers,  1/1 seqs), K (f16):  384.00 MiB, V (f16):  384.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 1560\n",
      "llama_context: worst-case: n_tokens = 8, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    8, n_seqs =  1, n_outputs =    8\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    8, n_seqs =  1, n_outputs =    8\n",
      "llama_context:        CPU compute buffer size =     2.88 MiB\n",
      "llama_context: graph nodes  = 1126\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
      "Model metadata: {'tokenizer.ggml.model': 'llama', 'phi3.feed_forward_length': '8192', 'general.name': 'Phi3', 'general.architecture': 'phi3', 'phi3.context_length': '4096', 'phi3.attention.head_count_kv': '32', 'phi3.embedding_length': '3072', 'phi3.block_count': '32', 'phi3.attention.head_count': '32', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.rope.dimension_count': '96', 'tokenizer.ggml.eos_token_id': '32000', 'general.file_type': '1', 'tokenizer.ggml.pre': 'default', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '32000', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\\n' + message['content'] + '<|end|>' + '\\n' + '<|assistant|>' + '\\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\\n'}}{% endif %}{% endfor %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\n",
      "' + message['content'] + '<|end|>' + '\n",
      "' + '<|assistant|>' + '\n",
      "'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\n",
      "'}}{% endif %}{% endfor %}\n",
      "Using chat eos_token: <|endoftext|>\n",
      "Using chat bos_token: <s>\n",
      "llama_perf_context_print:        load time =    1471.45 ms\n",
      "llama_perf_context_print: prompt eval time =    1471.04 ms /    10 tokens (  147.10 ms per token,     6.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =     219.79 ms /     1 runs   (  219.79 ms per token,     4.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1692.80 ms /    11 tokens\n",
      "llama_perf_context_print:    graphs reused =          0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=r\"C:\\Users\\ASUS\\Downloads\\Phi-3-mini-4k-instruct-fp16.gguf\",\n",
    "    n_gpu_layers=-1,   # ✅ all layers on GPU\n",
    "    max_tokens=500,\n",
    "    n_ctx=2048,\n",
    "    seed=42,\n",
    "    verbose=True       # set True to see loading logs\n",
    ")\n",
    "\n",
    "response = llm.invoke(\"Explain CUDA in one sentence.\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19afdfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1735.19 ms\n",
      "llama_perf_context_print: prompt eval time =     648.52 ms /    16 tokens (   40.53 ms per token,    24.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     649.62 ms /    17 tokens\n",
      "llama_perf_context_print:    graphs reused =          1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Hi! My name is Maarten. What is 1 + 1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99bc9cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Components\n",
    "persona = \"You are an expert in academic subjects and know everything in every field.\\n\"\n",
    "instruction = \"Use the conversation history if needed, answer the question asked and avoid repitative answer.\\n\"\n",
    "context = \"Your answer must be detailed and provide examples so the student can understand better.\\n\"\n",
    "data_format = (\n",
    "    \"Structure your answer as follows:\\n\"\n",
    "    \"1. Adapt the detail of your answer to the complexity of the question.\\n\"\n",
    "    \"2. Start with a definition or direct answer in 1-2 sentences.\\n\"\n",
    "    \"3. Add 1-3 sentences of explanation with an example if relevant.\\n\"\n",
    "    \"4. End with a short bullet-point summary of key takeaways.\\n\"\n",
    "    \"Keep total length around 3–5 sentences plus the bullets.\\n\"\n",
    ")\n",
    "audience = \"The answer is intended for students.\\n\"\n",
    "tone = \"The tone should be professional and clear.\\n\"\n",
    "question_component = \"Question to answer: {question}\\n\"\n",
    "\n",
    "# Combine into a single template with placeholders\n",
    "template = persona + instruction + context + data_format + audience + tone + question_component + \\\n",
    "           \"Conversation history:\\n{chat_history}\\n<|assistant|>\"\n",
    "\n",
    "# Define PromptTemplate\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"question\", \"chat_history\"]\n",
    ")\n",
    "\n",
    "# Conversation memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "# Create the LLMChain\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26926789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1 prefix-match hit, remaining 101 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1735.19 ms\n",
      "llama_perf_context_print: prompt eval time =    3753.26 ms /   101 tokens (   37.16 ms per token,    26.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =   65829.27 ms /   327 runs   (  201.31 ms per token,     4.97 tokens per second)\n",
      "llama_perf_context_print:       total time =   69843.64 ms /   428 tokens\n",
      "llama_perf_context_print:    graphs reused =        324\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Hi my name is ezam, provide me what is EDA in data analysis',\n",
       " 'chat_history': '',\n",
       " 'text': \" Hello Ezam, EDA stands for Exploratory Data Analysis. It is a crucial step in the data analysis process where we visually and quantitatively inspect datasets to uncover patterns, relationships, or anomalies within the data. This approach helps us gain insights into the underlying structure of the data before proceeding with more advanced statistical methods or predictive modeling techniques.\\n\\nEDA involves a range of graphical tools such as histograms, scatter plots, box plots, and heat maps to explore different aspects of data like distributions, correlations between variables, trends, and outliers. For example, in analyzing customer satisfaction survey data with multiple questions, we can create a pair plot (a combination of scatter plots) to visualize how responses to each question are related or if there's any correlation between them.\\n\\nIn addition to these graphical methods, EDA also includes some quantitative techniques like descriptive statistics (mean, median, mode), data summarization with measures such as standard deviation and variance, and hypothesis testing to make informed decisions about the dataset. By applying both visual and quantitative analysis during this initial step, analysts can identify potential issues in their datasets or discover trends that might not be readily apparent just by looking at raw numbers.\\n\\nUltimately, EDA serves as a foundation for making more accurate predictions using machine learning models, developing effective business strategies based on data-driven insights, and ultimately enhancing decision-making processes in various domains like finance, healthcare, marketing, etc.\"}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke({\n",
    "    \"question\":\"Hi my name is ezam, provide me what is EDA in data analysis\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4b586b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1 prefix-match hit, remaining 94 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1471.45 ms\n",
      "llama_perf_context_print: prompt eval time =    3425.93 ms /    94 tokens (   36.45 ms per token,    27.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =  109342.83 ms /   499 runs   (  219.12 ms per token,     4.56 tokens per second)\n",
      "llama_perf_context_print:       total time =  113307.51 ms /   593 tokens\n",
      "llama_perf_context_print:    graphs reused =        490\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Can you explain me about jsp and servlet',\n",
       " 'chat_history': '',\n",
       " 'text': \" Certainly! JSP (JavaServer Pages) and Servlet are both server-side technologies used to develop dynamic web applications in Java. They work together within the Jakarta EE (formerly Java EE) ecosystem, allowing developers to create interactive and responsive user interfaces for web pages.\\n\\nJSP is a technology that helps developers generate HTML, XML or other text-based markup languages dynamically by embedding Java code directly into an HTML page. It uses special tags like <% %>, which are called JSP scripting directives, to insert dynamic content in the generated output. In essence, JSP combines static resources (HTML/CSS) with dynamic elements produced by executing Java code within those pages.\\n\\nFor example, consider a simple JSP file that displays user information:\\n\\n```jsp\\n<!DOCTYPE html>\\n<html>\\n  <head>\\n    <title>User Information</title>\\n  </head>\\n  <body>\\n    <h1>Welcome, ${user.firstName} ${user.lastName}</h1>\\n    <p>Email: ${user.email}</p>\\n  </body>\\n</html>\\n```\\n\\nIn this JSP file, the curly braces `{}` enclose expressions that are replaced with actual values at runtime by a component called the Java Expression Language (EL) engine or simply an interpreter. For instance, `${user.firstName}` will be replaced with the value of the variable 'user' and its property 'firstName'.\\n\\nServlets, on the other hand, provide a way to handle requests and generate responses in more detail. They are Java programs that run on the server side and interact with HTTP-based clients (usually web browsers) by processing incoming requests and returning appropriate outputs as HTTP responses. A servlet typically has at least two methods: `init()` for initialization purposes, and `doGet()` or `doPost()` to handle specific request types like GET and POST.\\n\\nFor example, a simple Servlet class that displays user information might look like this:\\n\\n```java\\nimport javax.servlet.*;\\nimport javax.servlet.http.*;\\n\\npublic class UserServlet extends HttpServlet {\\n  @Override\\n  protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException {\\n    //\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke({\n",
    "    \"question\": \"Can you explain me about jsp and servlet\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73c76ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 77 prefix-match hit, remaining 538 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1471.45 ms\n",
      "llama_perf_context_print: prompt eval time =   21155.07 ms /   538 tokens (   39.32 ms per token,    25.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =   78147.89 ms /   344 runs   (  227.17 ms per token,     4.40 tokens per second)\n",
      "llama_perf_context_print:       total time =   99598.84 ms /   882 tokens\n",
      "llama_perf_context_print:    graphs reused =        382\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'In 2025, who is Malaysia Prime Minister',\n",
       " 'chat_history': \"Human: Can you explain me about jsp and servlet\\nAI:  Certainly! JSP (JavaServer Pages) and Servlet are both server-side technologies used to develop dynamic web applications in Java. They work together within the Jakarta EE (formerly Java EE) ecosystem, allowing developers to create interactive and responsive user interfaces for web pages.\\n\\nJSP is a technology that helps developers generate HTML, XML or other text-based markup languages dynamically by embedding Java code directly into an HTML page. It uses special tags like <% %>, which are called JSP scripting directives, to insert dynamic content in the generated output. In essence, JSP combines static resources (HTML/CSS) with dynamic elements produced by executing Java code within those pages.\\n\\nFor example, consider a simple JSP file that displays user information:\\n\\n```jsp\\n<!DOCTYPE html>\\n<html>\\n  <head>\\n    <title>User Information</title>\\n  </head>\\n  <body>\\n    <h1>Welcome, ${user.firstName} ${user.lastName}</h1>\\n    <p>Email: ${user.email}</p>\\n  </body>\\n</html>\\n```\\n\\nIn this JSP file, the curly braces `{}` enclose expressions that are replaced with actual values at runtime by a component called the Java Expression Language (EL) engine or simply an interpreter. For instance, `${user.firstName}` will be replaced with the value of the variable 'user' and its property 'firstName'.\\n\\nServlets, on the other hand, provide a way to handle requests and generate responses in more detail. They are Java programs that run on the server side and interact with HTTP-based clients (usually web browsers) by processing incoming requests and returning appropriate outputs as HTTP responses. A servlet typically has at least two methods: `init()` for initialization purposes, and `doGet()` or `doPost()` to handle specific request types like GET and POST.\\n\\nFor example, a simple Servlet class that displays user information might look like this:\\n\\n```java\\nimport javax.servlet.*;\\nimport javax.servlet.http.*;\\n\\npublic class UserServlet extends HttpServlet {\\n  @Override\\n  protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException {\\n    //\",\n",
       " 'text': \" I apologize for the confusion, but as of my last update in 2023, Najib Razak was not serving as Malaysia's Prime Minister. However, let me provide you with the current information up to that time:\\n\\nAs of 2025, it is important to check the most recent and accurate information on Malaysian politics since leadership positions can change over time due to elections or other political events. As of my last update in 2023, Mahathir Mohamad was serving as Prime Minister after his comeback win following an election victory for the Pakatan Harapan coalition, which he led. However, I would recommend checking the latest news and resources for the most up-to-date information on Malaysia's Prime Minister in 2025.\\n\\nIn the meantime, here is a brief explanation of how political leadership changes can occur:\\n\\nThe Prime Minister of Malaysia is elected by the majority party or coalition within the federal legislature known as the Parliament. After parliamentary elections are held, each state assembly member votes to choose their representative for the national House of Representatives. The candidate with the most votes becomes a Member of Parliament (MP). To become Prime Minister, an MP must be appointed and then formally approved by the Yang di-Pertuan Agong, Malaysia's constitutional monarch.\\n\\nPolitical changes may occur due to elections or resignations, which can result in a new leader being elected as Prime Minister. Therefore, it is crucial to stay updated with current events to know who holds this important position in 2025 and beyond.\"}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke({\n",
    "    \"question\":\"In 2025, who is Malaysia Prime Minister\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e1ae100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 50 prefix-match hit, remaining 125 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1471.45 ms\n",
      "llama_perf_context_print: prompt eval time =    4565.74 ms /   125 tokens (   36.53 ms per token,    27.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =   67455.41 ms /   314 runs   (  214.83 ms per token,     4.65 tokens per second)\n",
      "llama_perf_context_print:       total time =   72342.50 ms /   439 tokens\n",
      "llama_perf_context_print:    graphs reused =        313\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'what is python',\n",
       " 'chat_history': '',\n",
       " 'text': ' Python is a high-level, interpreted programming language known for its simplicity and readability. It was created by Guido van Rossum in the late 1980s and has since become one of the most popular languages worldwide.\\n\\nPython emphasizes code readability with its use of significant whitespace and avoidance of braces or semicolons, making it an excellent choice for beginners. It supports multiple programming paradigms including procedural, object-oriented, and functional programming approaches. An example of Python\\'s simplicity can be seen in the following print statement:\\n\\n```python\\nprint(\"Hello, World!\")\\n```\\n\\nPython has a large standard library that provides modules for various tasks such as web development (using frameworks like Django and Flask), data analysis (with libraries like Pandas and NumPy), and machine learning (through TensorFlow and Scikit-learn). Here is an example of using Python\\'s requests library to make a simple API call:\\n\\n```python\\nimport requests\\nresponse = requests.get(\\'https://api.github.com\\')\\nprint(response.status_code)\\n```\\n\\nSummary of key takeaways:\\n- Python is a versatile, high-level, and easy-to-learn programming language.\\n- Known for its simplicity in syntax, readability, and support for various programming paradigms.\\n- It has an extensive standard library to perform diverse tasks, like web development, data analysis, and machine learning.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke({\n",
    "    \"question\":\"what is python\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd742774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 165 prefix-match hit, remaining 342 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1471.45 ms\n",
      "llama_perf_context_print: prompt eval time =   12557.16 ms /   342 tokens (   36.72 ms per token,    27.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =   43284.07 ms /   194 runs   (  223.11 ms per token,     4.48 tokens per second)\n",
      "llama_perf_context_print:       total time =   55990.10 ms /   536 tokens\n",
      "llama_perf_context_print:    graphs reused =        218\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'what is 1+100000',\n",
       " 'chat_history': 'Human: what is python\\nAI:  Python is a high-level, interpreted programming language known for its simplicity and readability. It was created by Guido van Rossum in the late 1980s and has since become one of the most popular languages worldwide.\\n\\nPython emphasizes code readability with its use of significant whitespace and avoidance of braces or semicolons, making it an excellent choice for beginners. It supports multiple programming paradigms including procedural, object-oriented, and functional programming approaches. An example of Python\\'s simplicity can be seen in the following print statement:\\n\\n```python\\nprint(\"Hello, World!\")\\n```\\n\\nPython has a large standard library that provides modules for various tasks such as web development (using frameworks like Django and Flask), data analysis (with libraries like Pandas and NumPy), and machine learning (through TensorFlow and Scikit-learn). Here is an example of using Python\\'s requests library to make a simple API call:\\n\\n```python\\nimport requests\\nresponse = requests.get(\\'https://api.github.com\\')\\nprint(response.status_code)\\n```\\n\\nSummary of key takeaways:\\n- Python is a versatile, high-level, and easy-to-learn programming language.\\n- Known for its simplicity in syntax, readability, and support for various programming paradigms.\\n- It has an extensive standard library to perform diverse tasks, like web development, data analysis, and machine learning.',\n",
       " 'text': ' 1+100,000 equals 100,001.\\n\\nAdding two numbers is a fundamental arithmetic operation where you combine their values to get a total sum. In this case, you are adding the number one (1) to the large number ten hundred thousand (100,000).\\n\\nTo illustrate, imagine having 1 apple and receiving an additional 100,000 apples. The new total would be 100,001 apples. In mathematics notation: 1 + 100,000 = 100,001.\\n\\nKey takeaways:\\n- Adding two numbers results in a sum of their values (e.g., 1+100,000)\\n- This specific example equals 100,001 when adding one to ten hundred thousand.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke({\n",
    "    \"question\":\"what is 1+100000\"\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
